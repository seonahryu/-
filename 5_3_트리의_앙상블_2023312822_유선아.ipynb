{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zp6fW8MP-mrO"
      },
      "source": [
        "# 트리의 앙상블"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv1IwHmU-mrU"
      },
      "source": [
        "<table align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/rickiepark/hg-mldl/blob/master/5-3.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />구글 코랩에서 실행하기</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dIaIAizcRSG-"
      },
      "source": [
        "# 랜덤포레스트\n",
        "##RandomForestClassifier\n",
        "- 전체 특성 개수의 제곱근만큼의 특성 랜덤하게 선택하여 사용\n",
        "- 분류일 때는 각 트리의 클래스별 확률을 평균하여 가장 높은 확률을 가진 클래스를 예측함\n",
        "\n",
        "##RandomForestRegressor\n",
        "- 전체 특성 사용\n",
        "- 사이킷런의 랜덤포레스트는 기본적으로 100개(default)의 결정 트리 훈련\n",
        "- -> GridSearchCV, RandomSearch를 통해 적절한 하이퍼파라미터 탐색\n",
        "- 회귀일 때는 단순히 각 트리의 예측을 평균함"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioJUlZ0M_uSZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "#numpy, pandas, train_test_split import\n",
        "\n",
        "wine = pd.read_csv('https://bit.ly/wine_csv_data')\n",
        "#웹 상에 있는 데이터를 데이터프레임화 하여 wine에 저장\n",
        "\n",
        "data = wine[['alcohol', 'sugar', 'pH']].to_numpy()\n",
        "#alcohol, sugar, pH열을 data에 저장\n",
        "target = wine['class'].to_numpy()\n",
        "#class 열을 target에 저장\n",
        "\n",
        "train_input, test_input, train_target, test_target = train_test_split(data, target, test_size=0.2, random_state=42)\n",
        "#훈련 세트와 테스트 세트를 8:2로 나"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDKQudr7_8nu",
        "outputId": "e9dc5d13-d6ef-4c68-a5c0-52e7f97eaf50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9973541965122431 0.8905151032797809\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "#cross_validate, RandomForestClassifier 클래스 import\n",
        "\n",
        "rf = RandomForestClassifier(n_jobs=-1, random_state=42)\n",
        "#RandomForestClassifier 객체 rf 생성\n",
        "scores = cross_validate(rf, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "#cross_validate()에 rf, train_input, train_target을 인소로 전달\n",
        "#n_jobs=-1로 하여 모든 CPU 코어를 사용하고, 최대한 병렬로 교차 검증 수행하도록\n",
        "#return_train_score=True 를 통해 검증 점수뿐만 아니라 훈련 세트에 대한 점수도 같이 반환\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "#훈련 세트와 테스트 세트 점수의 평균 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYDbzXNLG8fK",
        "outputId": "c8b93973-9dbd-433b-f41c-dc0b79a6f94b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.23167441 0.50039841 0.26792718]\n"
          ]
        }
      ],
      "source": [
        "rf.fit(train_input, train_target)\n",
        "#rf 훈련\n",
        "\n",
        "print(rf.feature_importances_)\n",
        "#특성 중요도 출력\n",
        "#dT : alcohol 0.12, sugar 0.86, pH 0.007 -> 결정트리의 중요도와 다른 결과\n",
        "#랜덤포레스트가 특성의 일부를 랜덤하게 선택하여 결정트리를 훈련하기 때문에\n",
        "#하나의 특성에 과도하게 집중하지 않고, Feature 중요도가 덜 치우쳐져 좀 더 많은 특성 훈련에 기여함\n",
        "#overfitting 줄이고, 성능 높이는데 도움"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMc06S1Fa_A-",
        "outputId": "4a521cc0-651a-4d3d-be00-68403fdcd2b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8934000384837406\n"
          ]
        }
      ],
      "source": [
        "rf = RandomForestClassifier(oob_score=True, n_jobs=-1, random_state=42)\n",
        "#랜덤포레스트는 자체 model 평가 기능 갖추고 있음\n",
        "# -> oob : out of bags, 훈련에서 사용하지 않은 sample\n",
        "#oob_score=True를 통해 부트스트랩 샘플에 포함되지 않고 남은 샘플로 결정 트리 평가\n",
        "\n",
        "rf.fit(train_input, train_target)\n",
        "#rf 훈련\n",
        "\n",
        "print(rf.oob_score_)\n",
        "#각 졀정 트리의 OOB 점수를 평균하여 출력\n",
        "#oob 점수를 사용하면 교차 검증을 대신할 수 있어서 훈련 세트에 더 많은 샘플을 사용할 수 O"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdrVoeQZRU14"
      },
      "source": [
        "## 엑스트라트리\n",
        "- 랜덤포레스트와 비슷하게 동작\n",
        "- 부트스트랩 샘플 사용하지 않고, 각 결정트리 만들 때 전체 훈련 세트를 사용함 -> 속도 느려지지만 성능 높음\n",
        "- 노드 무작위로 분할(dT에서 splitter='random' 로 사용할 수 있으나, ExtraTreesClassifier 클래스 사용) -> 빠른 계산 속도\n",
        "- 엑스트라 트리는 무작위성이 좀 더 크기 때문에 랜덤포레스트보다 더 많은 결정트리를 훈련해야 함(랜덤포레스트 결정트리 default 100)\n",
        "- -> GridSearchCV, RandomSearch 통해서 적절한 파라미터 찾기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noMLdywdOGrE",
        "outputId": "eac10009-dfbb-4b2d-ca78-1a2bb721bb3e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9974503966084433 0.8887848893166506\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "#ExtraTreesClassifier 클래스 import\n",
        "\n",
        "et = ExtraTreesClassifier(n_jobs=-1, random_state=42)\n",
        "#ExtraTreesClassifier 의 객체 et 생성\n",
        "scores = cross_validate(et, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "#cross_validate()에 et, train_input, train_target을 인수로 전달\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "#훈련 세트와 테스트 세트 점수 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HnB0_mBqfcXL",
        "outputId": "dea60ed0-c89c-4b28-f36b-f1a632ce6da4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.20183568 0.52242907 0.27573525]\n"
          ]
        }
      ],
      "source": [
        "et.fit(train_input, train_target)\n",
        "print(et.feature_importances_)\n",
        "#et의 특성 중요도 출력"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csKxnaxeRX8s"
      },
      "source": [
        "## 그레이디언트 부스팅\n",
        "- 깊이가 얕은 결정 트리를 사용하여 이전 트리의 잔차(표본집단에서 예측과 관측값의 차이)를 보완하는 방식으로 앙상블하는 방법\n",
        "- GradientBoostingClassifier는 기본적으로 깊이가 3인 결정 트리 100개 사용\n",
        "- overfitting에 강하고, 높은 일반화 성능 기대 가능\n",
        "- 경사하강법으로 트리를 앙상블에 추가\n",
        "- 분류에서는 로지스틱 손실 함수 사용\n",
        "- 회귀에서는 평균 제곱 오차 함수 사용\n",
        "- 잔차를 활용하여 오차 함수 사용"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IlNEFkaNsoG",
        "outputId": "17aa4d4b-4f9e-46cc-da26-0fcb9b698e3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8881086892152563 0.8720430147331015\n"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "#GradientBoostingClassifier 클래스 import\n",
        "\n",
        "gb = GradientBoostingClassifier(random_state=42)\n",
        "#GradientBoostingClassifier 객체 gd 생성\n",
        "scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "#cross_validate()(교차검증)에 gb, train_ipnut, train_target 인수로 전달\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "#훈련 세트와 테스트 세트 점수 출력\n",
        "#overfitting 발생 X <- 깊이가 얕은 결정트리 사용하기 때문"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pNpeS8EWpeEi",
        "outputId": "4753ea63-ef6d-40c0-8e84-5306c56d3ec5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9464595437171814 0.8780082549788999\n"
          ]
        }
      ],
      "source": [
        "gb = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, random_state=42)\n",
        "scores = cross_validate(gb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "#성능 높이기 위해 n_estimators=500으로 하여 결정 트리 개수를 늘림\n",
        "#default 값 : n_estimators=100, (학습률 learning rate)lr=0.1\n",
        "#GridSearchCV, RandomSearch로 적절한 하이퍼 파라미터 찾는 것이 효율적\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "#훈련 세트와 테스트 세트 점수 출력"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qD6iWVsGqCAE",
        "outputId": "d621da25-66a8-4aef-ce4f-d85b9ac20228"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.15872278 0.68010884 0.16116839]\n"
          ]
        }
      ],
      "source": [
        "gb.fit(train_input, train_target)\n",
        "print(gb.feature_importances_)\n",
        "#gb 특성 중요도 출력\n",
        "#그레디언트 부스팅이 랜덤포레스트보다 일부 특성(당도)에 더 집중(잔차 보완에 중심을 맞춰서 중요한 feature에 focusing)\n",
        "#랜덤포레스트보다 조금 더 높은 성능\n",
        "#but, 순서대로 트리를 추가하기 때문에 훈련 속도 느림\n",
        "#subsample 매개변수 : 기본값은 1.0으로 전체 훈련 세트(gd)를 사용하지만,\n",
        "#1보다 작으면 훈련 세트의 일부를 사용해 확률적 경사 하강법(sgd), 미니배치 경사 하강법(mini-batch) 사용하여 속도 빠르게"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BthW_II9RbLa"
      },
      "source": [
        "## 히스토그램 기반 부스팅"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_3Ct_NNWQbdA",
        "outputId": "ce062886-1cf1-4169-e6e1-2e2d105eeba7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9321723946453317 0.8801241948619236\n"
          ]
        }
      ],
      "source": [
        "# 사이킷런 1.0 버전 아래에서는 다음 라인의 주석을 해제하고 실행하세요.\n",
        "# from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier\n",
        "\n",
        "hgb = HistGradientBoostingClassifier(random_state=42)\n",
        "scores = cross_validate(hgb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TvlB0GMTS3hn",
        "outputId": "62c06ffb-979f-4488-ff6b-3f42023d135a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.08876275 0.23438522 0.08027708]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "hgb.fit(train_input, train_target)\n",
        "result = permutation_importance(hgb, train_input, train_target, n_repeats=10,\n",
        "                                random_state=42, n_jobs=-1)\n",
        "print(result.importances_mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S8FfxInn-xBQ",
        "outputId": "fe562fce-1d79-4da9-962b-1d7b91b88899"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.05969231 0.20238462 0.049     ]\n"
          ]
        }
      ],
      "source": [
        "result = permutation_importance(hgb, test_input, test_target, n_repeats=10,\n",
        "                                random_state=42, n_jobs=-1)\n",
        "print(result.importances_mean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqplZjh0j2nw",
        "outputId": "a24b9e23-ca52-46ec-b360-647935214597"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8723076923076923"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "hgb.score(test_input, test_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8fz_FrezBezR"
      },
      "source": [
        "#### XGBoost\n",
        "- 히스토그램 기반 gradient boosting 알고리즘 라이브러리 (묶음 안에서 best split을 찾기 때문에 히스토그램 기반이라고 함)\n",
        "- feature를 256개의 구간으로 분할하여, 빠르게 최적의 분할을 찾음"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBYLvOiV6rga",
        "outputId": "58d75137-1c27-4156-a3d1-16a86b7ef197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9555033709953124 0.8799326275264677\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBClassifier\n",
        "#XGBClassifier 클래스 import\n",
        "\n",
        "xgb = XGBClassifier(tree_method='hist', random_state=42)\n",
        "#XGBClassifier의 객체 xgb 생성\n",
        "# tree_method(트리 빌딩 방법) : split point를 잡을 때, 히스토그램 사용 여부\n",
        "# auto, exact, hist, gpu_hist, approx -> 다양한 파라미터 => GridSearhCV, RandomSearch 활용~~\n",
        "# 데이터가 클수록 분할하는 것이 유리하나, 데이터가 작을 때는 오히려 단점이 될 수도\n",
        "#auto 데이터 크기에 따라 자동으로 조절 / exact 전수조사(작은 데이터) / hist 구간 나눠서\n",
        "#gpu_hist 로컬에 gpu가 있다면 좀 더 빠르게 훈련 / approx 가중치 고려하는 hist와 비슷한 방법\n",
        "#hist, gpu_hist 일반적으로 많이 사용\n",
        "\n",
        "scores = cross_validate(xgb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "#cross_validate()에 xgb, train_input, train_target을 인수로 전달\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "#훈련 세트와 테스트 세트 점수 출력"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl6nh6DOBd-B"
      },
      "source": [
        "#### LightGBM\n",
        "- 1) leaf wise method : 한 쪽 방향으로 성장\n",
        "- 2) 연관있는 feature combine -> light해서 빠름\n",
        "- MS에서 만든 히스토그램 기반 그레디언트 부스팅 라이브러리\n",
        "- 다양한 파라미터 => GridSearhCV, RandomSearch 활용~~"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maihlDMP7lmY",
        "outputId": "83921d16-47d4-4bca-c7ff-4548116da90b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.935828414851749 0.8801251203079884\n"
          ]
        }
      ],
      "source": [
        "from lightgbm import LGBMClassifier\n",
        "#LGBMClassifier 클래스 import\n",
        "lgb = LGBMClassifier(random_state=42)\n",
        "#LGBMClassifier 객체 lgb 생성\n",
        "scores = cross_validate(lgb, train_input, train_target, return_train_score=True, n_jobs=-1)\n",
        "#cross_validate()에 lgb, train_input, train_target을 인수로 전달\n",
        "\n",
        "print(np.mean(scores['train_score']), np.mean(scores['test_score']))\n",
        "#훈련 세트와 테스트 세트 점수 출력"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "default:Python",
      "language": "python",
      "name": "conda-env-default-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}