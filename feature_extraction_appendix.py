# -*- coding: utf-8 -*-
"""Feature_extraction_Appendix

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1z7Cb6rUWLuEwzuyDO82MG7r8n_ZHS3pE

# Feature extraction (text)

컴퓨터는 숫자로 모든 것을 연산


=> 그럼 텍스트, 이미지 등 비정형 데이터를 수치화 해줘야 합니다.


텍스트 같은 것을 어떻게 수치화를 시킬 수 있을까?

In machine learning,

1. Counvectorizer (BoW)

2. TF-IDF

3. word2vec

(co-occurrence, GloVe, fastText...)
"""

from google.colab import drive
drive.mount('/content/drive')

"""# 1. CountVectorizer (bag of words = BoW)"""

from sklearn.feature_extraction.text import CountVectorizer

corpus = [
'This is the first document.',
'This document is the second document.',
'And this is the third one.',
'Is this the first document?',
]

vectorizer1 = CountVectorizer(stop_words='english')
# 'stop_words' 에 포함되어 있는 단어 제거하여 데이터 처리 -> 제거된 단어는 중요도 낮음
vectorizer2 = CountVectorizer()

x_feature1 = vectorizer1.fit_transform(corpus)
x_feature2 = vectorizer2.fit_transform(corpus)
# fit(학습시키고)_transform(적용) -> 텍스트 데이터를 수치로 변환

print("stop words 적용: ", vectorizer1.get_feature_names_out())
print("stop words 미적용: ", vectorizer2.get_feature_names_out())
# 말뭉치 안에 어떤 단어가 있는지 알 수 O

print("vectorizer1.vocabulary_", vectorizer1.vocabulary_)
print("vectorizer2.vocabulary_", vectorizer2.vocabulary_)
# 각각의 단어가 어떤 숫자와 매칭되었는지 알 수 O

print(x_feature1.toarray())
# x_feature1 넘파이 형태로 정렬
# 첫번째 문장에서 'document(0)' 1번 등장, 'second(1)' 0번 등장 -> [1 0]
print("x_feature1.shape", x_feature1.shape)
# 말뭉치 크기(4개의 텍스트 데이터) 4 / 단어수 2

print()
print("*"*50)
print()

print(x_feature2.toarray())
print("x_feature2.shape", x_feature2.shape)
# 말뭉치 크기(4개의 텍스트 데이터) 4 / 단어수 9

"""# 2. TfidfVectorizer (Term Frequency - Inverse Document Frequency = TF-IDF)"""

from sklearn.feature_extraction.text import TfidfVectorizer

corpus = [
'This is the first document.',
'This document is the second document.',
'And this is the third one.',
'Is this the first document?',
]

tfidf_vectorizer1 = TfidfVectorizer(stop_words='english')
tfidf_vectorizer2 = TfidfVectorizer()

x_tfidf_feature1 = tfidf_vectorizer1.fit_transform(corpus)
x_tfidf_feature2 = tfidf_vectorizer2.fit_transform(corpus)

print("stop words 적용: ", tfidf_vectorizer1.get_feature_names_out())
print("stop words 미적용: ", tfidf_vectorizer2.get_feature_names_out())

print("tfidf_vectorizer1.vocabulary_", tfidf_vectorizer1.vocabulary_)
print("tfidf_vectorizer2.vocabulary_", tfidf_vectorizer2.vocabulary_)

print(x_tfidf_feature1.toarray())
# 빈도 뿐만 아니라 가중치 곱해줘서 float 형태의 값
print("x_tfidf_feature1.shape", x_tfidf_feature1.shape)

print()
print("*"*50)
print()

print(x_tfidf_feature2.toarray())
print("x_tfidf_feature2.shape", x_tfidf_feature2.shape)

"""# 3. Word2vec

사전학습을 통해서 단어 자체의 특징을 파악하고 이를 기반으로 벡터화
"""

!pip install nltk

import nltk
nltk.download('movie_reviews')
nltk.download('punkt')
# nltk에 내장되어 있는 데이터

# GoogleNews-vectors-negative300.bin (3.64GB)

from nltk.corpus import movie_reviews

sentences = [sentence for sentence in movie_reviews.sents()]
print("데이터의 개수 : ", len(sentences))

# 특징은 이전의 feature extraction은 string 타입으로 그대로 들어갔지만, word2vec은 문장이 분리된 리스트 형식으로 되어있다.
print("첫 번째 문장: ", sentences[0])
print("두 번째 문장: ", sentences[1])
print("세 번째 문장: ", sentences[2])

from gensim.models.word2vec import Word2Vec

model = Word2Vec(
    sentences, # 훈련 데이터
    size=100, # 임베딩되는 feature 차원, 밀집벡터 사용 -> 사용자가 지정한 벡터 안에서 임의의 숫자 배열을 갖도록 -> 메모리 효율적으로 사용
    window=3, # 앞, 뒤 참고한 window size
    min_count=2, # 최소 몇 개 이상의 단어만 학습 (빈도 수)
    sg=0 # 0:CBOW, 1: Skip-gram // Skip-gram 주로 사용함
         # CBOW : 주변 단어로부터 중심 단어 예측, Skip-gram : 중심 단어로부터 주변 단어 예측
)

model.wv["boy"] # 100개의 숫자 배열 <- vector값 100으로 지정했기 때문

model.wv["girl"]

model.wv.most_similar('boy') # boy와 연관된 단어 찾아줌

model.save("/content/drive/MyDrive/study.model")

model1 = Word2Vec.load('/content/drive/MyDrive/study.model')
# 이전에 사전 훈련시켰던 모델 로드해서 다음에 사용할 수 O

model1.wv['boy']

