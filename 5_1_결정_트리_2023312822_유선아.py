# -*- coding: utf-8 -*-
"""5-1 결정 트리_2023312822 유선아

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fFvOOs2rWuwi0o7M1Fye9qt9Eg52tBxp

# 결정 트리

<table align="left">
  <td>
    <a target="_blank" href="https://colab.research.google.com/github/rickiepark/hg-mldl/blob/master/5-1.ipynb"><img src="https://www.tensorflow.org/images/colab_logo_32px.png" />구글 코랩에서 실행하기</a>
  </td>
</table>

## 로지스틱 회귀로 와인 분류하기
"""

import pandas as pd

wine = pd.read_csv('https://bit.ly/wine_csv_data')
#데이터셋을 데이터프레임화

wine.head()
#처음 5개의 샘플 확인 (0 레드와인 / 1 화이트와인 -> 이진분류)

wine.info()
# wine 데이터프레임의 각 열의 데이터 타입과 누락된 데이터 있는지 확인
# 총 6497개의 샘플이 있고, 4개의 열은 모두 실숫값 float
#Non-Null -> 현재 누락X

wine.describe()
#평균, 표준편차, 최소, 최대값, 1사분위, 중간값, 3사분위 확인
#알코올도수, 당도, pH값의 스케일이 다름 -> StandardScaler 클래스로 특성 표준화
#alcohol, sugar, pH로 Class 분류

data = wine[['alcohol', 'sugar', 'pH']].to_numpy()
#wine 데이터프레임의 alcohol, sugar, pH 열을 넘파이 배열로 바꿔서 data에 저장
target = wine['class'].to_numpy()
#wine 데이터프레임의 class 열을 넘파이 배열로 바꿔서 target에 저장

from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    data, target, test_size=0.2, random_state=42)
#8:2 비율로 훈련 세트와 테스트 세트 나눔

print(train_input.shape, test_input.shape)
#훈련 세트와 테스트 세트의 크기 확인

from sklearn.preprocessing import StandardScaler
#알코올 도수와 당도, pH 값의 스케일이 다르므로 StandardScaler 클래스로 특성을 표준화함

ss = StandardScaler()
#StandardScaler의 객체 ss 생성
ss.fit(train_input)
#ss에 train_input 적용하여 모델 학습

train_scaled = ss.transform(train_input)
#표준점수로 변환된 train_scaled 생성
test_scaled = ss.transform(test_input)
#표준점수로 변환된 test_scaled 생성

from sklearn.linear_model import LogisticRegression
#로지스틱 회귀를 위한 LogisticRegression 클래스

lr = LogisticRegression()
#LogisticRegression의 객체 lr 생성
lr.fit(train_scaled, train_target)
#ir 훈련

print(lr.score(train_scaled, train_target))
#훈련 세트에 대한 점수 출력
print(lr.score(test_scaled, test_target))
#테스트 세트에 대한 점수 출력

#과소적합 -> 파라미터(규제 매개 변수) C값 크게 = 복잡하게 = 규제 약하게
#Solver 매개변수(하이퍼 파라미터)에서 다른 알고리즘 선택
#다항 특성 추가

"""### 설명하기 쉬운 모델과 어려운 모델"""

print(lr.coef_, lr.intercept_)
#로지스틱 회귀 계수와 절편 출력
#이 모델이 왜 저런 계수 값을 학습했는지 정확히 이해X 추측만 -> 로지스틱 회귀 모델 이해 어려움
#다항 특성 추가하면 설명하기 더 어려워질 것

"""## 결정 트리

- 결정 트리 모델은 스무고개와 비슷, 질문을 하나씩 던져서 정답과 맞춰감
- 데이터를 잘 나눌 수 있는 질문을 찾는다면, 계속 질문을 추가해서 분류 정확도를 높일 수 있음
- 사이킷런의 DecisionTreeClassifier 클래스 사용하여 모델 훈련
"""

from sklearn.tree import DecisionTreeClassifier
#결정 트리를 위한 DecisionTreeClassifier 클래스 사용

dt = DecisionTreeClassifier(random_state=42)
#DecisionTreeClassifier의 객체 dt 생성
#random_state 설정 필수X, 어디서든 모델 결과가 같게 나올 수 있도록

dt.fit(train_scaled, train_target)
#dt 훈련

print(dt.score(train_scaled, train_target))
#훈련 세트에 대한 점수 출력
print(dt.score(test_scaled, test_target))
#테스트 세트에 대한 점수 출력

#테스트 세트의 성능이 훈련 세트에 비해 낮음 -> overfitting

import matplotlib.pyplot as plt
from sklearn.tree import plot_tree

plt.figure(figsize=(10,7))
plot_tree(dt)
plt.show()
#결정 트리 모델 객체를 그림으로 출력
#결정 트리는 위에서 아래로 자라남
#맨 위의 노드 - 루트노드 / 맨아래 끝의 노드 - 리프노드

plt.figure(figsize=(10,7))
plot_tree(dt, max_depth=1, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
#max_depth로 노드 깊이 2로 설정
#filled=True로 클래스에 맞게 노드의 색을 채움,  the color indicates to which class the majority of the samples at each node belong to.
#feature_name으로 특성의 이름 전달

plt.show()
#그림 출력

#samples : 샘플 개수
#value : 음성 클래스(레드와인) 1258, 1(양성) 클래스(화이트와인) 3939
# -> 해당 조건 sugar<=-0.239을 가진 것은 화이트와인이다.
#결정 트리의 예측 : 리프 노드에서 가장 많은 클래스 = 예측클래스
#만약 여기서 성장을 멈춘다면, 왼쪽 노드에 도달한 샘플과 오른쪽 노드에 도달한 샘플은 모두 양성 클래스(화이트와인)로 예측

"""### 가지치기"""

dt = DecisionTreeClassifier(max_depth=3, random_state=42)
#노드 깊이를 3으로 설정
dt.fit(train_scaled, train_target)
#dt 훈련

print(dt.score(train_scaled, train_target))
#훈련 세트에 대한 점수 출력
print(dt.score(test_scaled, test_target))
#테스트 세트에 대한 점수 출력

plt.figure(figsize=(20,15))
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()

#지니계수(Gini Index)에서 상대빈도 사용하기 때문
#특성값의 스케일은 결정 트리 알고리즘에 아무런 영향을 미치지 않음
#표준화 전처리 할 필요 X

dt = DecisionTreeClassifier(max_depth=3, random_state=42)
#노드 깊이 3으로 설정
dt.fit(train_input, train_target)
#dt를 표준화되지 않은 데이터로 훈련

print(dt.score(train_input, train_target))
#훈련 세트에 대한 점수 출력
print(dt.score(test_input, test_target))
#테스트 세트에 대한 점수 출력

plt.figure(figsize=(20,15))
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
#파란색 - 1로 예측(화이트와인) / 주황색 - 0 예측(레드와인)
#진한 주황색의 경우(오른쪽 yes, 왼쪽 No) : 1.624<sugar<=4.325, alcohol<=11.025
plt.show()
#그림 출력 => 특성값을 표준점수로 바꾸지 않아 이해하기 쉬움

print(dt.feature_importances_)
#특성 중요도 출력
#feature_names=['alcohol', 'sugar', 'pH'] -> sugar가 가장 중요한 특성이었음을 알 수 있음

"""## 확인문제"""

dt = DecisionTreeClassifier(min_impurity_decrease=0.0005, random_state=42)
# min을 줌으로써 이정도도 안되는 모델은 찾지말라는 뜻이다. (복잡해야, 균질해야)
#최소 불순도 min_impurity_decrease
dt.fit(train_input, train_target)

print(dt.score(train_input, train_target))
print(dt.score(test_input, test_target))

plt.figure(figsize=(20,15))
plot_tree(dt, filled=True, feature_names=['alcohol', 'sugar', 'pH'])
plt.show()