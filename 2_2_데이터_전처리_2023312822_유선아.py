# -*- coding: utf-8 -*-
"""2-2. 데이터 전처리_2023312822 유선아

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ruTIKbEYR1qtOUc_b-3OO6VbY7rwCqXx

# 데이터 전처리
# -> data scailing의 중요성, 큰 값에 치우칠 수 있으므로 min/max 또는 표준화 필요

<table align="left">
  <td>
    <a target="_blank" href="https://colab.research.google.com/github/rickiepark/hg-mldl/blob/master/2-2.ipynb"><img src="https://www.tensorflow.org/images/colab_logo_32px.png" />구글 코랩에서 실행하기</a>
  </td>
</table>

## 넘파이로 데이터 준비하기
"""

fish_length = [25.4, 26.3, 26.5, 29.0, 29.0, 29.7, 29.7, 30.0, 30.0, 30.7, 31.0, 31.0,
                31.5, 32.0, 32.0, 32.0, 33.0, 33.0, 33.5, 33.5, 34.0, 34.0, 34.5, 35.0,
                35.0, 35.0, 35.0, 36.0, 36.0, 37.0, 38.5, 38.5, 39.5, 41.0, 41.0, 9.8,
                10.5, 10.6, 11.0, 11.2, 11.3, 11.8, 11.8, 12.0, 12.2, 12.4, 13.0, 14.3, 15.0]
# 생선의 길이(도미, 빙어)

fish_weight = [242.0, 290.0, 340.0, 363.0, 430.0, 450.0, 500.0, 390.0, 450.0, 500.0, 475.0, 500.0,
                500.0, 340.0, 600.0, 600.0, 700.0, 700.0, 610.0, 650.0, 575.0, 685.0, 620.0, 680.0,
                700.0, 725.0, 720.0, 714.0, 850.0, 1000.0, 920.0, 955.0, 925.0, 975.0, 950.0, 6.7,
                7.5, 7.0, 9.7, 9.8, 8.7, 10.0, 9.9, 9.8, 12.2, 13.4, 12.2, 19.7, 19.9]
# 생선의 무게(도미, 빙어)

import numpy as np
# numpy를 import

np.column_stack(([1,2,3], [4,5,6]))
# 예시 : column_stack() 함수는 매개변수로 받은 리스트를 일렬로 세운 다음, 차례대로 나란히 연결, 연결할 리스트는 튜플 형태로 전달

fish_data = np.column_stack((fish_length, fish_weight))
# fish_length와 fish_weight를 column_stack() 함수를 사용하여 하나의 쌍으로 x 데이터 묶음

print(fish_data[:5])
# 0~4번째 인덱스 데이터 출력

print(np.ones(5))
# 모든 원소가 1인 5개의 원소를 넘파이 배열로 출력
# np.zeros()는 np.ones()와 유사하게 원소가 0인 것을 넘파이 배열로 출력 ex) np.zeros(5) // [0. 0. 0. 0. 0.]

fish_target = np.concatenate((np.ones(35), np.zeros(14)))
# 1이 35개인 배열과 0이 14개인 배열을 만든 다음, np.concatenate() 함수 사용
# np.concatenate() 는 첫번째 차원을 따라 배열을 연결함 (cf. column_stack())

print(fish_target)
# fish_target 출력

"""## 사이킷런으로 훈련 세트와 테스트 세트 나누기"""

from sklearn.model_selection import train_test_split
# train_test_split 함수는 배열을 비율에 맞게 훈련 세트와 테스트 세트로 나눠줌
# 이전처럼 직접 인덱스를 섞어서 훈련 세트와 테스트 세트로 나눌 필요 없음

train_input, test_input, train_target, test_target = train_test_split(
    fish_data, fish_target, random_state=42)
# fish_data(x)를 train_input(x train)과 test_input(x test)으로 2개의 배열로 나누고
# fish_target(y)을 train_input(y train)과 test_input(y test)으로 2개의 배열로 나눔
# random_state는 랜덤시드와 같은 역할, 임의의 값 설정
# 기본적으로 25%를 테스트 세트로 설정함 (일반적으로 train:test을 75:25 또는 80:20)

print(train_input.shape, test_input.shape)
# train_input과 test_input 형태 출력

print(train_target.shape, test_target.shape)
# train_target과 test_target 형태 출력
# target은 0 또는 1 배열이므로 1차원(1dim)

print(test_target)
# 도미와 빙어가 잘 섞였는지 알기 위해 test_target 출력해봄
# 13개 테스트 세트 중에 도미(1)가 10, 빙어(0)가 3으로 3.3:1
# 원래 비율은 35:14=2.5:1으로 샘플링 편향
# 샘플링 편향이 있어 훈련 세트와 테스트 세트에 비율이 일정하지 않아 모델이 일부 샘플을 올바르게 학습할 수 없음
# stratify 매개변수에 타켓 데이터를 전달하면 클래스 비율에 맞게 데이터를 나눔 (stratify는 train_test_split에서 쓸 수 있음)

train_input, test_input, train_target, test_target = train_test_split(
    fish_data, fish_target, stratify=fish_target, random_state=42)
# stratify 매개변수에 타깃 데이터 전달

print(test_target)
# test_target 출력, 전체 훈련 데이터 비율과 동일하지 않지만 2.25:1로 유사해짐

"""## 수상한 도미 한마리"""

from sklearn.neighbors import KNeighborsClassifier
# 사이킷런에서 KNeighborsClassifier를 import

kn = KNeighborsClassifier()
# 객체 선언
# ()에 값을 넣느냐에 따라서 data(x,y) 저장 / parameter(기본값) 사용

kn.fit(train_input, train_target)
# train_input과 train_target 데이터로 모델 학습

kn.score(test_input, test_target)
# score()으로 성능 평가

print(kn.predict([[25, 150]]))
# 도미의 데이터를 넣고 결과 확인
# 결과는 빙어(0) <- 성능 100% 나왔지만, 알고자 하는 데이터 즉, unseen data 넣으니까 원하는 결과 X - predict 함수 활용해서 알 수 있음

import matplotlib.pyplot as plt
# matplotlib 라이브러리로 시각화

plt.scatter(train_input[:,0], train_input[:,1])
# 도미와 빙어 데이터 산점도

plt.scatter(25, 150, marker='^')
# 샘플 주황색 세모 표시

plt.xlabel('length')
# x축 라벨

plt.ylabel('weight')
# y축 라벨

plt.show()
# 출력

distances, indexes = kn.kneighbors([[25, 150]])
# KNeighborsClassifier 클래스는 주어진 샘플에서 가장 가까운 이웃을 찾아줌
# kneighbors() 이웃까지의 거리와 이웃 샘플의 인덱스 반환
# KNeighborsClassifier 클래스의 이웃 개수인 n_neighbors의 기본값은 5이므로 5개의 이웃이 반환됨

plt.scatter(train_input[:,0], train_input[:,1])
# 도미와 빙어 데이터 산점도

plt.scatter(25, 150, marker='^')
# 샘플 주황색 세모 표시

plt.scatter(train_input[indexes,0], train_input[indexes,1], marker='D')
# marker='D'를 통해, 다이아몬드로 indexs에 있는 원소에 해당하는 데이터를 산점도에 표시

plt.xlabel('length')
# x축 라벨

plt.ylabel('weight')
# y축 라벨

plt.show()
# 출력 // 빙어 4, 도미 1 => scale 맞춰줘야 거리 제대로 계산할 수 있음

print(train_input[indexes])
# train_input[indexs] 데이터 출력 // 빙어 4, 도미 1

print(train_target[indexes])
# train_target[indexs] 데이터 출력 // 빙어 4, 도미 1

print(distances)
# kneighbors()에서 반환한 distances 배열 출력

"""## 기준을 맞춰라"""

plt.scatter(train_input[:,0], train_input[:,1])
plt.scatter(25, 150, marker='^')
plt.scatter(train_input[indexes,0], train_input[indexes,1], marker='D')
plt.xlim((0, 1000))
# x축 범위 0~1000
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
# x축과 y축 범위를 동일하게 맞추니, 데이터가 수직으로 늘어선 형태 -> 생선의 길이(x축)은 가장 가까운 이웃을 찾는데 크게 영향 X
# 오로지 생선의 무게(y축)만을 고려 => 특성값을 일정한 기준으로 맞추는 데이터 전처리 필요
# min-max 스케일링 / 표준화 (데이터 전처리) 중 하나인 표준점수(standard score) 사용

mean = np.mean(train_input, axis=0)
# np.mean() 평균 계산

std = np.std(train_input, axis=0)
# np.std() 표준편차 계산

# axis=0 행, axis=1 열
# 행을 기준으로 각 열의 통계값 계산

print(mean, std)
# 평균, 표준편차 출력

train_scaled = (train_input - mean) / std
# 표준 점수를 알기 위해 원본 데이터에서 평균을 빼고 표준편차로 나눔

"""## 전처리 데이터로 모델 훈련하기"""

plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(25, 150, marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
# 샘플[25,150]을 표준점수로 변환하지 않았기 때문에 주황색 세모가 덩그러니 떨어져 있음

new = ([25, 150] - mean) / std
# 표준점수화

plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(new[0], new[1], marker='^')
plt.xlabel('length')
plt.ylabel('weight')
plt.show()
# 표준점수화 한 데이터 다시 출력
# x축과 y축의 범위가 -1.5~1.5로 바뀜

kn.fit(train_scaled, train_target)
# 모델 훈련

test_scaled = (test_input - mean) / std
# 테스트 세트도 표준점수화하여 데이터 전처리 진행
# training에서 사용한 기준이 맞는지 보려는 것이기 때문에 이를 기반으로 mean, std 사용
# test 는 건드리지X 새로운 기준(평균, 표준편차 등) 만들지X

kn.score(test_scaled, test_target)
# score() 성능 평가
# 1.0으로 100% 정확도

print(kn.predict([new]))
# 이전에 빙어(0)가 나온 데이터 다시 적용하니 도미(1) 출력

distances, indexes = kn.kneighbors([new])
# kneighbors로 k-최근접 이웃 구함

plt.scatter(train_scaled[:,0], train_scaled[:,1])
plt.scatter(new[0], new[1], marker='^')
plt.scatter(train_scaled[indexes,0], train_scaled[indexes,1], marker='D')
# 샘플과 가까운 5개의 이웃을 녹색 다이아몬드로 출력

plt.xlabel('length')
plt.ylabel('weight')
plt.show()
# 가장 가까운 이웃에 변화가 생김. 도미 5개

"""- knn(k-최근접 이웃) 분류 알고리즘 - class 예측, 주변 이웃 class 다수결
- knn 분류 알고리즘 - 수치값 예측, 주변 이웃 값의 평균 (3-1)
"""