{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming과 Lemmatization을 쓰는 이유\n",
        "\n",
        "단어의 종류가 너무 많다...\n",
        "\n",
        "이전 Feature extraction의 예시를 보면, 빈도수로 사전을 만드는 경우(BoW, TF-IDF)가 존재\n",
        "단어의 개수가 많아지면, 단어 count문제 및 메모리 문제가 발생\n",
        "\n",
        "-> 복잡성을 줄여주자!!\n",
        "\n",
        "ex) like, likes / watch, watched ...\n",
        "\n",
        "서로 다른 단어 처럼 보이지만, 본래의 뜻은 같으므로 일반화 할 수 있음\n",
        "\n",
        "===========================================================================\n",
        "\n",
        "like => like\n",
        "\n",
        "likes => like\n",
        "\n",
        "===========================================================================\n",
        "\n",
        "watch => watch\n",
        "\n",
        "watched => watch\n",
        "\n",
        "===========================================================================\n",
        "\n",
        "일반화 과정을 통해서, 단어의 수를 줄일 수 있음\n",
        "\n",
        "전처리 과정에서 적용 (in machine learning or topic modeling)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "a5lTOiZZ4pD0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Stemming (어간 추출)\n",
        "\n",
        "stem (어간)은 단어의 의미를 담고 있는 핵심적인 부분.\n",
        "\n",
        "단어에서 변화하지 않는 부분을 이르는 말.\n",
        "\n",
        "stemming (어간 추출)은 정해진 규칙을 보고 어미를 자르는 작업. 조금 러프한 작업.\n",
        "\n",
        "* 어간 추출 후 나오는 단어는 사전에 존재하지 않는 단어가 될 수 있는 가능성이 존재.\n",
        "\n",
        "\n",
        "\n",
        "Ref: https://wikidocs.net/21707"
      ],
      "metadata": {
        "id": "MAPXEdwl8JmP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tS_sSUqHljqN",
        "outputId": "357a0f4b-09e0-4e5c-aae4-664a52e7fec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "어간 추출 전 : ['BERT', 'is', 'designed', 'to', 'pretrain', 'deep', 'bidirectional', 'representations', 'from', 'unlabeled', 'text', 'by', 'jointly', 'conditioning', 'on', 'both', 'left', 'and', 'right', 'context', 'in', 'all', 'layers', '.', 'As', 'a', 'result', ',', 'the', 'pre-trained', 'BERT', 'model', 'can', 'be', 'finetuned', 'with', 'just', 'one', 'additional', 'output', 'layer', 'to', 'create', 'state-of-the-art', 'models', 'for', 'a', 'wide', 'range', 'of', 'tasks', ',', 'such', 'as', 'question', 'answering', 'and', 'language', 'inference', ',', 'without', 'substantial', 'taskspecific', 'architecture', 'modifications', '.']\n",
            "어간 추출 후 : ['bert', 'is', 'design', 'to', 'pretrain', 'deep', 'bidirect', 'represent', 'from', 'unlabel', 'text', 'by', 'jointli', 'condit', 'on', 'both', 'left', 'and', 'right', 'context', 'in', 'all', 'layer', '.', 'as', 'a', 'result', ',', 'the', 'pre-train', 'bert', 'model', 'can', 'be', 'finetun', 'with', 'just', 'one', 'addit', 'output', 'layer', 'to', 'creat', 'state-of-the-art', 'model', 'for', 'a', 'wide', 'rang', 'of', 'task', ',', 'such', 'as', 'question', 'answer', 'and', 'languag', 'infer', ',', 'without', 'substanti', 'taskspecif', 'architectur', 'modif', '.']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "# 문장을 단어, 형태소 단위로 나눠서 분석하기 위해 word_tokenize 모듈 사용\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "sentence = ''' BERT is designed to pretrain deep bidirectional representations from\n",
        "unlabeled text by jointly conditioning on both\n",
        "left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer\n",
        "to create state-of-the-art models for a wide\n",
        "range of tasks, such as question answering and\n",
        "language inference, without substantial taskspecific architecture modifications.'''\n",
        "\n",
        "tokenized_sentence = word_tokenize(sentence)\n",
        "# 문장을 단어 단위로 분할\n",
        "\n",
        "print()\n",
        "print('어간 추출 전 :', tokenized_sentence)\n",
        "print('어간 추출 후 :',[stemmer.stem(word) for word in tokenized_sentence])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Lemmatization (표제어 추출)\n",
        "\n",
        "Lemma (표제어)는 사전형 단어이다. 본래 같은 근간을 가지는 단어이면 다른 형태라도 같게 취급을 함.\n",
        "\n",
        "표제어 추출은 단어의 근간이 되는 단어를 찾아서 단어의 개수를 줄이는 것을 목적으로 함.\n",
        "\n",
        "단어의 형태학적 파싱을 먼저 진행하는 것. 형태소의 종류는 어간(stem)과 접사(affix)가 존재, 형태학적 파싱은 이 두 가지 구성 요소를 분리하는 작업\n",
        "\n",
        "1. 어간: 단어의 의미를 담고 있는 단어의 핵심 부분.\n",
        "\n",
        "2. 접사: 단어에 추가적인 의미를 주는 부분.\n",
        "\n",
        "ex) cat(어간) s(접사)로 분리\n",
        "\n",
        "Ref: https://wikidocs.net/21707"
      ],
      "metadata": {
        "id": "vw0JDGq18ont"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "sentence = ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'had', 'starting', 'was', 'is', 'am']\n",
        "# 분할된 단어를 넣어도 O\n",
        "\n",
        "print('표제어 추출 전 :', sentence)\n",
        "print('표제어 추출 후 :', [lemmatizer.lemmatize(word) for word in sentence])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LKN4no6xOvF9",
        "outputId": "6d3da350-802d-4a22-ab06-2e65b5b4e643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "표제어 추출 전 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'lives', 'fly', 'dies', 'watched', 'has', 'had', 'starting', 'was', 'is', 'am']\n",
            "표제어 추출 후 : ['policy', 'doing', 'organization', 'have', 'going', 'love', 'life', 'fly', 'dy', 'watched', 'ha', 'had', 'starting', 'wa', 'is', 'am']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MJjQJM5PThtd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}