# -*- coding: utf-8 -*-
"""4-2 SGDClassifier(확률적 경사 하강법)_2023312822 유선아

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1KmxiB0XLs3g1bqrix9dL8HTW0DqElM3z

# 확률적 경사 하강법

<table align="left">
  <td>
    <a target="_blank" href="https://colab.research.google.com/github/rickiepark/hg-mldl/blob/master/4-2.ipynb"><img src="https://www.tensorflow.org/images/colab_logo_32px.png" />구글 코랩에서 실행하기</a>
  </td>
</table>

## SGDClassifier
"""

import pandas as pd

fish = pd.read_csv('https://bit.ly/fish_csv_data')
# fish_csv_data 파일에서 데이터프레임화

fish_input = fish[['Weight','Length','Diagonal','Height','Width']].to_numpy()
# Species열을 제외한 나머지 5개 입력 데이터(x)로 사용

fish_target = fish['Species'].to_numpy()
# Species열을 타깃 데이터(y)로 사용

from sklearn.model_selection import train_test_split

train_input, test_input, train_target, test_target = train_test_split(
    fish_input, fish_target, random_state=42)
# 훈련 세트와 테스트 세트로 나눔

from sklearn.preprocessing import StandardScaler
# StandardScaler 클래스 import -> 데이터 전처리(Scailing or MinMax)

ss = StandardScaler()
# 객체 생성

ss.fit(train_input)
# 모델 훈련 -> 데이터 관찰

train_scaled = ss.transform(train_input)
# 훈련 세트 변환

test_scaled = ss.transform(test_input)
# 테스트 세트 변환 / test 세트는 변환만. fit 함수 쓰지 X (train set 기준 사용)

from sklearn.linear_model import SGDClassifier
# SGDClassifier 클래스 import

sc = SGDClassifier(loss='log_loss', max_iter=10, random_state=42)
# 기존 머신러닝 모델에 SGDClassifier() 적용하면 SGD 적용 가능
# 객체 생성하면서, 손실함수로 'log(로지스틱 손실 함수)'
# max_iter=10으로 에포크 횟수를 10으로 설정 (max_iter=epoch 에포크=data 처음부터 끝까지 관찰)
# SGD가 랜덤으로 값 뽑아서 쓰는 것이므로 고정된 성능을 원한다면 random_state 설정

sc.fit(train_scaled, train_target)
# 모델 훈련

print(sc.score(train_scaled, train_target))
# 훈련 세트 score 출력

print(sc.score(test_scaled, test_target))
# 테스트 세트 score 출력 -> underfitting 발생
# 반복 횟수 10번이 부족하여 max_iter 크게 하라는 경고 -> partial_fit() 활용

sc.partial_fit(train_scaled, train_target)
# 모델 이어서 훈련시에는 partial_fir() 메서드 이용
# 호출할 때마다 1epoch씩 이어서 훈련 => max_iter=10 +1 = 11
# partial_fit() 메서드를 통해 점진적 학습 진행 (모델 sc 추가로 더 훈련O)

print(sc.score(train_scaled, train_target))
# 훈련 세트 score 출력

print(sc.score(test_scaled, test_target))
# 테스트 세트 score 출력
# 아직 점수 낮지만, 정확도 향상됨. -> max_iter 크게 해야

"""## 에포크와 과대/과소적합"""

import numpy as np

sc = SGDClassifier(loss='log_loss', random_state=42)
# 객체 생성하면서, 손실함수로 'log(로지스틱 손실 함수)'
# max_iter default value = 1000 사용
## 반복학습의 숫자, max_iter=epoch는 하이퍼 파라미터(임의로 직접 지정)

train_score = []
# 훈련 세트 score 저장할 리스트 생성

test_score = []
# 테스트 세트 score 저장할 리스트 생성

classes = np.unique(train_target)
# train_target에 있는 7개 생선의 목록(y) 생성

# 300번 에포크동안 훈련 반복
# _ -> 반복 기능만 사용, index 필요 없어서
for _ in range(0, 300):
    sc.partial_fit(train_scaled, train_target, classes=classes)
    # fit(y 관찰) 쓰고 partial_fit 쓰면 y 정보 다시 안 줘도 O
    # partial_fit 먼저 쓰는 경우, y 정보 알려줘야 -> classes=classes (y값 정보)

    train_score.append(sc.score(train_scaled, train_target))
    # 반복당 훈련 세트 score를 train_score 리스트에 추가

    test_score.append(sc.score(test_scaled, test_target))
    # 반복당 테스트 세트 score를 test_score 리스트에 추가

import matplotlib.pyplot as plt

plt.plot(train_score)
plt.plot(test_score)
plt.xlabel('epoch')
# x축 라벨
plt.ylabel('accuracy')
# y축 라벨
plt.show()
# epoch 적절한 값 찾는 방법
# 1) 시각화, train_score와 test_score 간격 최소일 때
# 2) |train_score - test_score| 최소일 때
# 100번째 epoch 이후 훈련 세트와 테스트 세트의 점수가 조금씩 벌어지고 있음 => 100번째 epoch가 적절한 반복 횟수
# 에포크 초기에는 underfitting

sc = SGDClassifier(loss='log_loss', max_iter=100, tol=None, random_state=42)
# 객체 생성하면서 반복횟수(epoch) 100으로 설정
# tol=None으로 지정하여 자동으로 멈추지 X (tol : 조기종료 시켜주는 parameter)

sc.fit(train_scaled, train_target)
# 모델 학습

print(sc.score(train_scaled, train_target))
# 훈련 세트 score 출력
print(sc.score(test_scaled, test_target))
# 테스트 세트 score 출력

sc = SGDClassifier(loss='hinge', max_iter=100, tol=None, random_state=42)
# 객체 생성하면서, 손실함수 'hinge' 설정 (loss 매개변수의 기본값은 hinge, 힌지 손실은 SVM(서포트 벡터 머신)을 위한 손실함수 >> 이 값을 이용하면, SVM 방식으로 분류)

sc.fit(train_scaled, train_target)
# 모델 학습

print(sc.score(train_scaled, train_target))
# 훈련 세트 score 출력
print(sc.score(test_scaled, test_target))
# 테스트 세트 score 출력